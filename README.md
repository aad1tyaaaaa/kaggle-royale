# kaggle-royale
The pipeline is moving forward. Here is the current progress:

‚úÖ Completed Steps
Data Ingestion & Validation: Successfully validated all 5 datasets. Handled specific Windows encoding and data type issues (e.g., coercing avg_elixir_player which had mixed string/float values).
Preprocessing & Feature Engineering: Created a robust set of 135 features, including difference features (player vs opponent), ratio features, and interaction terms (Patch x Synergy, etc.).
LightGBM Training: Completed 5-fold CV with an OOF log-loss of 0.688406.
LGB Submission: Saved the first baseline submission: sub01_lgb_baseline.csv.
Resume Logic: I implemented a resume feature that skips already-trained folds. Fold 1 and Fold 2 of CatBoost have already been processed (Fold 1 was loaded from disk, Fold 2 finished training recently).
‚è≥ Current Status: CatBoost Training (Stage 7b)
Active Fold: Fold 3 of 5.
Fold Progress: Currently at iteration ~235/2000.
Validation Performance: Fold 1 and 2 showed very consistent log-loss (~0.6883).
Estimated Remaining Time: Each CatBoost fold is taking roughly 15-20 minutes on this environment without early stopping. I expect the full pipeline (including calibration and ensembling) to complete in approximately 1.5 to 2 hours.
üìä Preliminary Observations
Feature Importance: The top features driving the LightGBM model are diff_tempo1, diff_tempo2, pd_win_condition_family, and diff_skill_ceiling. This confirms that deck archetype dynamics and skill-gap features are highly predictive.
Stability: The log-loss is quite stable across folds, which is a good sign of a robust model that won't overfit to the public LB.
I am continuing to monitor the background process. I will provide a final update once sub06_stacking.csv (the final ensemble) is generated.